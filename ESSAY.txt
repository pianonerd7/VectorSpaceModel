1. In this assignment, we didn't ask you to support phrasal queries, which is a feature that is typically 
supported in web search engines. Describe how you would support phrasal search in conjunction with the 
VSM model. A sketch of the algorithm is sufficient.
(For those of you who like a challenge, please go ahead and implement this feature in your submission but 
clearly demarcate it in your code and allow this feature to be turned on or off using the command line 
switch "-x" (where "-x" means to turn on the extended processing of phrasal queries). We will give a small 
bonus to submissions that achieve this functionality correctly).

We can use positional indices to help. So instead of what we are doing now, which is indexing the document
for each term, we do that but also record the position in the document. In our dictionary, we used a node.py
class to represent each dictionary value, we keep doing that, which keeps track of the term and the document
frequency, but we should also create an object post_node.py wheere it stores the document_id of the term as 
well as the offset of the positional index. When we have a phrasal query like "good sleep", then we just make 
sure that within the top ranked documents, good is at position x, then sleep is at position x + 1 within
that same document and that the position of good is before sleep. 


2. Describe how your search engine reacts to long documents and long queries as compared to short documents 
and queries. Is the normalization you use sufficient to address the problems (see Section 6.4.4 for a hint)? 
In your judgement, is the ltc.lnc scheme (n.b., not the ranking scheme you were asked to implement) 
sufficient for retrieving documents from the Reuters-21578 collection?

Long documents will have more words, and probably also more unique terms. Similarly, short documents will likely
have less unique terms, and when a query like "arachnocentric definition" contains a specific word like "arachnocentric",
and a longer document contains that word, then that document will surely rank better than a shorter document
that only has the word "definition". 

For short queries, we know that idf has no effect on single word queries. For longer queries I expect a more
accurate rank list. My logic is that for shorter queries with a couple words, out of a large collection, it is
likely that a lot of documents will contain those words, but for a longer query, the cosine of score would only
rank documents that closely resemble the query (as in, at least containing all the words in the query) optimally.

Additionally, since when we are indexing the documents, it needs to read the entire document and parse each word
in the document. And because a longer document contains more words, it would take a longer time to index as 
compared to a shorter document.

Our normalization disregards the document length and basically post normalization weighs documents of longer size
as documents of shorter size. I think this is insignificant because as long as the user gets the documents that 
fit their need, then it is fine. We don't need to ensure the absolutely best result. That also seem to be a very
subjective thing to go after, since the user can be querying for anything.

I believe the ltc.lnc scheme is sufficient. The only possible wrinkle is the lnc, where we don't account for 
low frequency words in the query. But I think it is insignificant, since the likelyhood have having a very long
and verbose query is low for the reuters-21578.


3. Do you think zone or field parametric indices would be useful for practical search in the Reuters collection? 
Note: the Reuters collection does have metadata for each article but the quality of the metadata is not uniform, 
nor are the metadata classifications uniformly applied (some documents have it, some don't). Hint: for the 
next Homework #4, we will be using field metadata, so if you want to base Homework #4 on your Homework #3, 
you're welcomed to start support of this early (although no extra credit will be given if it's right).

Yes, zone indices might be useful for practical search in the Reuters collecion. For most documents in the 
collection, there's a metadata about the title of the document. Thus, we can have one zone index for title 
and one zone index for body. It may help users to find documents they want more easily. For example a journalist 
might be interested in documents that contain keywords in their titles, but don't care if keywords are in body. 
In fact, most traditional retrieval systems, such as library portals, use zone indices.

Also, we can apply weighted zone scoring. For example, when there's a query, we apply VSM on both title zone 
index and body zone index, and get scores from both of them. We can learn the weights for them by using 
machined-learned relevance methodology. The final score would be 
title_weight * title_score + (1 - title_weight) * body_score. If we find appropriate weights, the accuracy 
of relevance ranking will be improved.

However, field parametric indices may not be useful. Because there's rarely field parametric metadata in 
the Reuters collection, and those metadata are not uniformed. If we use parametric metadata, because field 
queries are typically treated as conjunction and there's few documents that contain those metadata, few 
documents will be retrieved and the recall should be quite low.
